# é…ç½®æ–‡ä»¶ç®¡ç†
## æŠ€èƒ½é…ç½®ã€ç¯å¢ƒå˜é‡å’Œç”¨æˆ·è®¾ç½®ç®¡ç†

## ğŸ¯ é…ç½®ç®¡ç†æ¦‚è¿°

### 1. é…ç½®å±‚çº§ä½“ç³»
```
å¤šå±‚é…ç½®æ¶æ„:
â€¢ é»˜è®¤é…ç½®: æŠ€èƒ½å†…ç½®çš„é»˜è®¤å€¼
â€¢ ç¯å¢ƒé…ç½®: ç¯å¢ƒå˜é‡è¦†ç›–
â€¢ ç”¨æˆ·é…ç½®: ç”¨æˆ·è‡ªå®šä¹‰é…ç½®
â€¢ è¿è¡Œæ—¶é…ç½®: ç¨‹åºè¿è¡Œæ—¶åŠ¨æ€é…ç½®
â€¢ ä¼šè¯é…ç½®: å•ä¸ªä¼šè¯çš„ä¸´æ—¶é…ç½®
```

### 2. é…ç½®ç®¡ç†åŸåˆ™
```
é…ç½®ç®¡ç†æœ€ä½³å®è·µ:
â€¢ å®‰å…¨æ€§: æ•æ„Ÿä¿¡æ¯åŠ å¯†å­˜å‚¨
â€¢ å¯ç»´æŠ¤æ€§: é…ç½®ç»“æ„æ¸…æ™°æ˜“æ‡‚
â€¢ å¯æ‰©å±•æ€§: æ”¯æŒåŠ¨æ€æ·»åŠ é…ç½®
â€¢ å…¼å®¹æ€§: å‘åå…¼å®¹é…ç½®å˜æ›´
â€¢ å¯æµ‹è¯•æ€§: æ”¯æŒé…ç½®å•å…ƒæµ‹è¯•
```

## âš™ï¸ é…ç½®æ–‡ä»¶è®¾è®¡

### 1. é…ç½®æ–‡ä»¶æ ¼å¼
#### JSONé…ç½®ç¤ºä¾‹ï¼š
```json
{
  "chinese_toolkit": {
    "version": "1.0.0",
    "api": {
      "baidu_translate": {
        "enabled": true,
        "app_id": "${BAIDU_APP_ID}",
        "app_key": "${BAIDU_APP_KEY}",
        "endpoint": "https://fanyi-api.baidu.com/api/trans/vip/translate"
      },
      "tencent_cloud": {
        "enabled": false,
        "secret_id": "${TENCENT_SECRET_ID}",
        "secret_key": "${TENCENT_SECRET_KEY}"
      }
    },
    "local_services": {
      "ocr": {
        "enabled": true,
        "language": "chi_sim",
        "timeout": 30
      },
      "translation": {
        "enabled": true,
        "cache_size": 1000,
        "cache_ttl": 3600
      }
    },
    "performance": {
      "max_workers": 4,
      "timeout": 30,
      "retry_attempts": 3
    },
    "logging": {
      "level": "INFO",
      "file": "chinese_toolkit.log",
      "max_size_mb": 10
    }
  }
}
```

#### YAMLé…ç½®ç¤ºä¾‹ï¼š
```yaml
chinese_toolkit:
  version: "1.0.0"
  
  api:
    baidu_translate:
      enabled: true
      app_id: "${BAIDU_APP_ID}"
      app_key: "${BAIDU_APP_KEY}"
      endpoint: "https://fanyi-api.baidu.com/api/trans/vip/translate"
    
    tencent_cloud:
      enabled: false
      secret_id: "${TENCENT_SECRET_ID}"
      secret_key: "${TENCENT_SECRET_KEY}"
  
  local_services:
    ocr:
      enabled: true
      language: "chi_sim"
      timeout: 30
    
    translation:
      enabled: true
      cache_size: 1000
      cache_ttl: 3600
  
  performance:
    max_workers: 4
    timeout: 30
    retry_attempts: 3
  
  logging:
    level: "INFO"
    file: "chinese_toolkit.log"
    max_size_mb: 10
```

### 2. é…ç½®éªŒè¯æ¨¡å¼
#### ä½¿ç”¨Pydanticè¿›è¡Œé…ç½®éªŒè¯ï¼š
```python
from pydantic import BaseModel, Field, validator
from typing import Optional, Dict, List
from enum import Enum

class LogLevel(str, Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"

class APIConfig(BaseModel):
    enabled: bool = True
    app_id: Optional[str] = None
    app_key: Optional[str] = None
    endpoint: str
    timeout: int = 30
    
    @validator('app_id', 'app_key')
    def check_api_keys(cls, v, values):
        if values.get('enabled') and not v:
            raise ValueError("APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        return v

class LocalServiceConfig(BaseModel):
    enabled: bool = True
    cache_size: int = Field(1000, ge=0, le=10000)
    cache_ttl: int = Field(3600, ge=0, le=86400)

class ChineseToolkitConfig(BaseModel):
    version: str
    api: Dict[str, APIConfig]
    local_services: Dict[str, LocalServiceConfig]
    performance: Dict[str, int]
    logging: Dict[str, str]
    
    class Config:
        env_prefix = "CHINESE_TOOLKIT_"
        env_file = ".env"
```

## ğŸ”§ é…ç½®ç®¡ç†å®ç°

### 1. é…ç½®åŠ è½½å™¨
#### å¤šæºé…ç½®åŠ è½½ï¼š
```python
# config_loader.py
import json
import yaml
import os
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv

class ConfigLoader:
    def __init__(self, config_name: str = "chinese_toolkit"):
        self.config_name = config_name
        self.config = {}
        self._load_environment()
    
    def _load_environment(self):
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        load_dotenv()
        
        # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
        env_config = {}
        for key, value in os.environ.items():
            if key.startswith(f"{self.config_name.upper()}_"):
                config_key = key[len(self.config_name) + 1:].lower()
                env_config[config_key] = value
        
        self.config.update(self._parse_env_config(env_config))
    
    def _parse_env_config(self, env_config: Dict) -> Dict:
        """è§£æç¯å¢ƒå˜é‡é…ç½®"""
        parsed = {}
        for key, value in env_config.items():
            keys = key.split('__')
            current = parsed
            
            for i, k in enumerate(keys):
                if i == len(keys) - 1:
                    # å°è¯•è½¬æ¢ç±»å‹
                    if value.lower() in ['true', 'false']:
                        current[k] = value.lower() == 'true'
                    elif value.isdigit():
                        current[k] = int(value)
                    else:
                        current[k] = value
                else:
                    if k not in current:
                        current[k] = {}
                    current = current[k]
        
        return parsed
    
    def load_file(self, file_path: str) -> Dict:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        path = Path(file_path)
        
        if not path.exists():
            return {}
        
        with open(path, 'r', encoding='utf-8') as f:
            if path.suffix == '.json':
                file_config = json.load(f)
            elif path.suffix in ['.yaml', '.yml']:
                file_config = yaml.safe_load(f)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼: {path.suffix}")
        
        # åˆå¹¶é…ç½®ï¼ˆæ–‡ä»¶é…ç½®ä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡ï¼‰
        self._merge_config(self.config, file_config)
        return self.config
    
    def _merge_config(self, base: Dict, update: Dict) -> None:
        """é€’å½’åˆå¹¶é…ç½®"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._merge_config(base[key], value)
            else:
                base[key] = value
    
    def get(self, key: str, default: Any = None) -> Any:
        """è·å–é…ç½®å€¼"""
        keys = key.split('.')
        current = self.config
        
        for k in keys:
            if isinstance(current, dict) and k in current:
                current = current[k]
            else:
                return default
        
        return current
    
    def set(self, key: str, value: Any) -> None:
        """è®¾ç½®é…ç½®å€¼"""
        keys = key.split('.')
        current = self.config
        
        for i, k in enumerate(keys):
            if i == len(keys) - 1:
                current[k] = value
            else:
                if k not in current:
                    current[k] = {}
                current = current[k]
```

### 2. é…ç½®çƒ­é‡è½½
#### ç›‘æ§é…ç½®æ–‡ä»¶å˜åŒ–ï¼š
```python
# config_watcher.py
import time
import threading
from pathlib import Path
from typing import Callable, Optional
import hashlib

class ConfigWatcher:
    def __init__(self, config_file: str, callback: Callable):
        self.config_file = Path(config_file)
        self.callback = callback
        self.last_hash = self._get_file_hash()
        self.watching = False
        self.thread = None
    
    def _get_file_hash(self) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œå€¼"""
        if not self.config_file.exists():
            return ""
        
        with open(self.config_file, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _watch_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.watching:
            current_hash = self._get_file_hash()
            
            if current_hash != self.last_hash:
                self.last_hash = current_hash
                self.callback(self.config_file)
            
            time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
    
    def start(self):
        """å¼€å§‹ç›‘æ§"""
        if not self.watching:
            self.watching = True
            self.thread = threading.Thread(target=self._watch_loop, daemon=True)
            self.thread.start()
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.watching = False
        if self.thread:
            self.thread.join(timeout=2)
```

## ğŸ” å®‰å…¨é…ç½®ç®¡ç†

### 1. æ•æ„Ÿä¿¡æ¯åŠ å¯†
#### ä½¿ç”¨åŠ å¯†å­˜å‚¨ï¼š
```python
# secure_config.py
import base64
import json
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
import os

class SecureConfig:
    def __init__(self, password: str, salt: Optional[bytes] = None):
        self.password = password.encode()
        self.salt = salt or os.urandom(16)
        self.fernet = self._create_fernet()
    
    def _create_fernet(self) -> Fernet:
        """åˆ›å»ºFernetåŠ å¯†å™¨"""
        kdf = PBKDF2(
            algorithm=hashes.SHA256(),
            length=32,
            salt=self.salt,
            iterations=100000
        )
        key = base64.urlsafe_b64encode(kdf.derive(self.password))
        return Fernet(key)
    
    def encrypt_config(self, config: Dict) -> str:
        """åŠ å¯†é…ç½®"""
        config_json = json.dumps(config).encode()
        encrypted = self.fernet.encrypt(config_json)
        return base64.urlsafe_b64encode(encrypted).decode()
    
    def decrypt_config(self, encrypted_config: str) -> Dict:
        """è§£å¯†é…ç½®"""
        encrypted = base64.urlsafe_b64decode(encrypted_config.encode())
        decrypted = self.fernet.decrypt(encrypted)
        return json.loads(decrypted.decode())
    
    def save_secure(self, config: Dict, file_path: str):
        """ä¿å­˜åŠ å¯†é…ç½®"""
        encrypted = self.encrypt_config(config)
        
        # ä¿å­˜ç›å€¼å’ŒåŠ å¯†æ•°æ®
        data = {
            'salt': base64.urlsafe_b64encode(self.salt).decode(),
            'encrypted_config': encrypted
        }
        
        with open(file_path, 'w') as f:
            json.dump(data, f)
    
    def load_secure(self, file_path: str) -> Dict:
        """åŠ è½½åŠ å¯†é…ç½®"""
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        self.salt = base64.urlsafe_b64decode(data['salt'].encode())
        self.fernet = self._create_fernet()
        
        return self.decrypt_config(data['encrypted_config'])
```

### 2. å¯†é’¥ç®¡ç†
#### ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ï¼š
```python
# key_manager.py
import keyring
import json
from typing import Optional

class KeyManager:
    def __init__(self, service_name: str = "chinese_toolkit"):
        self.service_name = service_name
    
    def store_api_key(self, api_name: str, key_data: Dict):
        """å­˜å‚¨APIå¯†é’¥"""
        keyring.set_password(
            self.service_name,
            api_name,
            json.dumps(key_data)
        )
    
    def get_api_key(self, api_name: str) -> Optional[Dict]:
        """è·å–APIå¯†é’¥"""
        key_json = keyring.get_password(self.service_name, api_name)
        if key_json:
            return json.loads(key_json)
        return None
    
    def delete_api_key(self, api_name: str):
        """åˆ é™¤APIå¯†é’¥"""
        keyring.delete_password(self.service_name, api_name)
    
    def list_api_keys(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰APIå¯†é’¥"""
        # æ³¨æ„ï¼škeyringå¯èƒ½ä¸æ”¯æŒç›´æ¥åˆ—å‡ºï¼Œè¿™é‡Œä½¿ç”¨è‡ªå®šä¹‰å­˜å‚¨
        import sqlite3
        import os
        
        # æŸ¥æ‰¾keyringæ•°æ®åº“
        keyring_path = os.path.expanduser("~/.local/share/python_keyring/keyring_pass.cfg")
        if os.path.exists(keyring_path):
            conn = sqlite3.connect(keyring_path)
            cursor = conn.cursor()
            cursor.execute(
                "SELECT username FROM credentials WHERE service_name = ?",
                (self.service_name,)
            )
            return [row[0] for row in cursor.fetchall()]
        return []
```

## ğŸ“Š é…ç½®åˆ†æå’Œä¼˜åŒ–

### 1. é…ç½®ä½¿ç”¨åˆ†æ
#### è·Ÿè¸ªé…ç½®ä½¿ç”¨æƒ…å†µï¼š
```python
# config_analytics.py
import time
from collections import defaultdict
from typing import Dict, List
import json

class ConfigAnalytics:
    def __init__(self):
        self.access_count = defaultdict(int)
        self.access_time = defaultdict(list)
        self.config_values = {}
    
    def track_access(self, config_key: str, value: Any):
        """è·Ÿè¸ªé…ç½®è®¿é—®"""
        self.access_count[config_key] += 1
        self.access_time[config_key].append(time.time())
        self.config_values[config_key] = value
    
    def get_usage_report(self) -> Dict:
        """è·å–ä½¿ç”¨æŠ¥å‘Š"""
        report = {
            'total_accesses': sum(self.access_count.values()),
            'most_accessed': [],
            'least_accessed': [],
            'access_patterns': {}
        }
        
        # è®¡ç®—æœ€å¸¸è®¿é—®çš„é…ç½®
        sorted_access = sorted(
            self.access_count.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        report['most_accessed'] = sorted_access[:10]
        report['least_accessed'] = sorted_access[-10:] if len(sorted_access) > 10 else []
        
        # åˆ†æè®¿é—®æ¨¡å¼
        for key, times in self.access_time.items():
            if len(times) > 1:
                intervals = [times[i+1] - times[i] for i in range(len(times)-1)]
                report['access_patterns'][key] = {
                    'count': len(times),
                    'avg_interval': sum(intervals) / len(intervals),
                    'min_interval': min(intervals),
                    'max_interval': max(intervals)
                }
        
        return report
    
    def suggest_optimizations(self) -> List[Dict]:
        """æä¾›ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        report = self.get_usage_report()
        
        # å»ºè®®1: ç¼“å­˜é¢‘ç¹è®¿é—®çš„é…ç½®
        for key, count in report['most_accessed']:
            if count > 100:
                suggestions.append({
                    'type': 'cache',
                    'config_key': key,
                    'reason': f'é¢‘ç¹è®¿é—® ({count}æ¬¡)',
                    'recommendation': 'æ·»åŠ å†…å­˜ç¼“å­˜'
                })
        
        # å»ºè®®2: åˆå¹¶ç›¸å…³é…ç½®
        related_keys = defaultdict(list)
        for key in self.config_values.keys():
            prefix = key.split('.')[0]
            related_keys[prefix].append(key)
        
        for prefix, keys in related_keys.items():
            if len(keys) > 5:
                suggestions.append({
                    'type': 'consolidate',
                    'config_prefix': prefix,
                    'reason': f'ç›¸å…³é…ç½®è¿‡å¤š ({len(keys)}ä¸ª)',
                    'recommendation': 'è€ƒè™‘åˆå¹¶ç›¸å…³é…ç½®'
                })
        
        return suggestions
```

### 2. é…ç½®æ€§èƒ½ä¼˜åŒ–
#### é…ç½®ç¼“å­˜ç­–ç•¥ï¼š
```python
# config_cache.py
import time
from functools import lru_cache
from typing import Any, Callable

class ConfigCache:
    def __init__(self, ttl: int = 300):
        self.ttl = ttl
        self.cache = {}
        self.timestamps = {}
    
    def get(self, key: str, loader: Callable[[], Any]) -> Any:
        """è·å–é…ç½®å€¼ï¼Œæ”¯æŒç¼“å­˜"""
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if (key in self.cache and 
            key in self.timestamps and
            current_time - self.timestamps[key] < self.ttl):
            return self.cache[key]
        
        # åŠ è½½æ–°å€¼
        value = loader()
        self.cache[key] = value
        self.timestamps[key] = current_time
        return value
    
    def invalidate(self, key: str = None):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        if key:
            self.cache.pop(key, None)
            self.timestamps.pop(key, None)
        else:
            self.cache.clear()
            self.timestamps.clear()
    
    @lru_cache(maxsize=128)
    def get_cached(self, key: str) -> Any:
        """ä½¿ç”¨LRUç¼“å­˜è·å–é…ç½®"""
        # è¿™é‡Œå‡è®¾é…ç½®å·²ç»åŠ è½½åˆ°å†…å­˜ä¸­
        return self._get_from_source(key)
    
    def _get_from_source(self, key: str) -> Any:
        """ä»æºè·å–é…ç½®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“è¯»å–
        time.sleep(0.01)  # æ¨¡æ‹ŸIOå»¶è¿Ÿ
        return f"value_for_{key}"
```

## ğŸ”„ é…ç½®è¿ç§»å’Œç‰ˆæœ¬ç®¡ç†

### 1. é…ç½®ç‰ˆæœ¬è¿ç§»
#### è‡ªåŠ¨é…ç½®è¿ç§»ï¼š
```python
# config_migration.py
import json
from typing import Dict, Any
from pathlib import Path

class ConfigMigrator:
    def __init__(self, migrations_dir: str = "migrations"):
        self.migrations_dir = Path(migrations_dir)
        self.migrations = self._load_migrations()
    
    def _load_migrations(self) -> Dict[str, Callable]:
        """åŠ è½½è¿ç§»è„šæœ¬"""
        migrations = {}
        
        if self.migrations_dir.exists():
            for migration_file in self.migrations_dir.glob("*.py"):
                migration_name = migration_file.stem
                
                # åŠ¨æ€å¯¼å…¥è¿ç§»æ¨¡å—
                import importlib.util
                spec = importlib.util.spec_from_file_location(
                    migration_name,
                    migration_file
                )
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                if hasattr(module, 'migrate'):
                    migrations[migration_name] = module.migrate
        
        return migrations
    
    def migrate_config(self, config: Dict, from_version: str, to_version: str) -> Dict:
        """è¿ç§»é…ç½®"""
        # è·å–éœ€è¦åº”ç”¨çš„è¿ç§»
        migrations_to_apply = self._get_migrations_between(from_version, to_version)
        
        # æŒ‰é¡ºåºåº”ç”¨è¿ç§»
        migrated_config = config.copy()
        for migration_name in migrations_to_apply:
            if migration_name in self.migrations:
                migrated_config = self.migrations[migration_name](migrated_config)
        
        return migrated_config
    
    def _get_migrations_between(self, from_version: str, to_version: str) -> List[str]:
        """è·å–ä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´çš„è¿ç§»åˆ—è¡¨"""
        # è¿™é‡Œéœ€è¦å®ç°ç‰ˆæœ¬æ¯”è¾ƒå’Œè¿ç§»é¡ºåºé€»è¾‘
        # ç®€åŒ–å®ç°ï¼šå‡è®¾è¿ç§»æ–‡ä»¶ååŒ…å«ç‰ˆæœ¬ä¿¡æ¯
        migration_files = sorted(self.migrations_dir.glob("*.py"))
        migrations = []
        
        for migration_file in migration_files:
            # æå–ç‰ˆæœ¬ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼šv1_0_to_v1_1.pyï¼‰
            version_part = migration_file.stem
            if version_part.startswith('v'):
                migrations.append(version_part)
        
        return migrations

### 2. é…ç½®å¤‡ä»½å’Œæ¢å¤
#### è‡ªåŠ¨é…ç½®å¤‡ä»½ï¼š
```python
# config_backup.py
import shutil
import json
from datetime import datetime
from pathlib import Path
from typing import List

class ConfigBackup:
    def __init__(self, backup_dir: str = "backups", max_backups: int = 10):
        self.backup_dir = Path(backup_dir)
        self.max_backups = max_backups
        self.backup_dir.mkdir(exist_ok=True)
    
    def create_backup(self, config_file: str, description: str = ""):
        """åˆ›å»ºé…ç½®å¤‡ä»½"""
        config_path = Path(config_file)
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{config_path.stem}_{timestamp}.bak"
        backup_path = self.backup_dir / backup_name
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(config_path, backup_path)
        
        # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
        metadata = {
            'original_file': str(config_path),
            'backup_time': timestamp,
            'description': description,
            'file_size': config_path.stat().st_size,
            'checksum': self._calculate_checksum(config_path)
        }
        
        metadata_path = backup_path.with_suffix('.meta.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)
        
        # æ¸…ç†æ—§å¤‡ä»½
        self._cleanup_old_backups()
        
        return str(backup_path)
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        import hashlib
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        backups = list(self.backup_dir.glob("*.bak"))
        if len(backups) > self.max_backups:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
            backups.sort(key=lambda x: x.stat().st_mtime)
            for backup in backups[:-self.max_backups]:
                backup.unlink()
                # åŒæ—¶åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                meta_file = backup.with_suffix('.meta.json')
                if meta_file.exists():
                    meta_file.unlink()
    
    def list_backups(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        for backup_file in self.backup_dir.glob("*.bak"):
            meta_file = backup_file.with_suffix('.meta.json')
            metadata = {}
            
            if meta_file.exists():
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            
            backups.append({
                'backup_file': str(backup_file),
                'metadata': metadata
            })
        
        return sorted(backups, key=lambda x: x['metadata'].get('backup_time', ''), reverse=True)
    
    def restore_backup(self, backup_file: str, target_file: str = None):
        """æ¢å¤å¤‡ä»½"""
        backup_path = Path(backup_file)
        if not backup_path.exists():
            raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
        
        # è¯»å–å…ƒæ•°æ®è·å–åŸå§‹æ–‡ä»¶å
        meta_file = backup_path.with_suffix('.meta.json')
        if meta_file.exists():
            with open(meta_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            original_file = metadata.get('original_file', target_file)
        else:
            original_file = target_file
        
        if not original_file:
            raise ValueError("éœ€è¦æŒ‡å®šç›®æ ‡æ–‡ä»¶æˆ–å¤‡ä»½åŒ…å«åŸå§‹æ–‡ä»¶å")
        
        # æ¢å¤æ–‡ä»¶
        shutil.copy2(backup_path, original_file)
        return original_file
```

## ğŸš€ æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†æœ€ä½³å®è·µ
```
è®¾è®¡åŸåˆ™:
â€¢ å•ä¸€èŒè´£: æ¯ä¸ªé…ç½®é¡¹æœ‰æ˜ç¡®çš„ç›®çš„
â€¢ æœ€å°æƒé™: åªæš´éœ²å¿…è¦çš„é…ç½®é¡¹
â€¢ é»˜è®¤å®‰å…¨: é»˜è®¤é…ç½®åº”è¯¥æ˜¯å®‰å…¨çš„
â€¢ æ˜ç¡®æ–‡æ¡£: æ¯ä¸ªé…ç½®é¡¹éƒ½æœ‰è¯¦ç»†è¯´æ˜

å®ç°åŸåˆ™:
â€¢ ç±»å‹å®‰å…¨: ä½¿ç”¨ç±»å‹æç¤ºå’ŒéªŒè¯
â€¢ ç¯å¢ƒéš”ç¦»: ä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒé…ç½®
â€¢ ç‰ˆæœ¬æ§åˆ¶: é…ç½®å˜æ›´å¯è¿½æº¯
â€¢ è‡ªåŠ¨åŒ–æµ‹è¯•: é…ç½®å˜æ›´è‡ªåŠ¨æµ‹è¯•
```

### 2. é…ç½®éƒ¨ç½²ç­–ç•¥
```
å¼€å‘ç¯å¢ƒ:
â€¢ ä½¿ç”¨æœ¬åœ°é…ç½®æ–‡ä»¶
â€¢ åŒ…å«ç¤ºä¾‹é…ç½®
â€¢ æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
â€¢ æä¾›å¿«é€Ÿå¯åŠ¨é…ç½®

æµ‹è¯•ç¯å¢ƒ:
â€¢ ä½¿ç”¨ç‹¬ç«‹çš„é…ç½®æ–‡ä»¶
â€¢ åŒ…å«æµ‹è¯•ä¸“ç”¨é…ç½®
â€¢ æ”¯æŒé…ç½®éªŒè¯
â€¢ è‡ªåŠ¨åŒ–é…ç½®éƒ¨ç½²

ç”Ÿäº§ç¯å¢ƒ:
â€¢ ä½¿ç”¨åŠ å¯†é…ç½®æ–‡ä»¶
â€¢ æ”¯æŒçƒ­é‡è½½
â€¢ åŒ…å«ç›‘æ§å’Œå‘Šè­¦
â€¢ æ”¯æŒå›æ»šæœºåˆ¶
```

### 3. é…ç½®ç›‘æ§å’Œå‘Šè­¦
```
ç›‘æ§æŒ‡æ ‡:
â€¢ é…ç½®åŠ è½½æ—¶é—´
â€¢ é…ç½®è®¿é—®é¢‘ç‡
â€¢ é…ç½®å˜æ›´æ¬¡æ•°
â€¢ é…ç½®é”™è¯¯ç‡

å‘Šè­¦è§„åˆ™:
â€¢ é…ç½®åŠ è½½å¤±è´¥
â€¢ é…ç½®éªŒè¯é”™è¯¯
â€¢ æ•æ„Ÿé…ç½®å˜æ›´
â€¢ é…ç½®æ€§èƒ½ä¸‹é™
```

---
**é…ç½®æ–‡ä»¶ç®¡ç†æŒ‡å—ç‰ˆæœ¬**: 1.0.0
**æœ€åæ›´æ–°**: 2026-02-23
**é€‚ç”¨å¯¹è±¡**: æŠ€èƒ½å¼€å‘è€…ã€ç³»ç»Ÿç®¡ç†å‘˜

**ä¼˜ç§€é…ç½®ï¼Œç¨³å®šè¿è¡Œï¼** âš™ï¸ğŸ”§

**å®‰å…¨ç¬¬ä¸€ï¼Œæ€§èƒ½è‡³ä¸Šï¼** ğŸ›¡ï¸ğŸš€