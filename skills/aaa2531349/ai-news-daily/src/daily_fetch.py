"""
AI News Aggregator - å›½å†… AI åª’ä½“æŠ“å–ï¼ˆæ”¹è¿›ç‰ˆï¼‰
ä¼˜åŒ–ç‚¹ï¼š
1. æ›´ä¸¥æ ¼çš„AIç›¸å…³æ€§åˆ¤æ–­ï¼ˆæ ‡é¢˜å¿…é¡»å«AIå…³é”®è¯ï¼‰
2. è¿‡æ»¤ç»¼åˆæ—©æŠ¥/æ™¨æŠ¥ç±»æ–‡ç« 
3. æŠ“å–æ­£æ–‡è·å–çœŸå®æ‘˜è¦
4. æŒ‰AIç›¸å…³åº¦æ’åº
"""
import requests
import feedparser
import json
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# å›½å†… AI åª’ä½“æº
SOURCES = {
    "qbitai": {
        "name": "é‡å­ä½",
        "rss": "https://www.qbitai.com/feed",
        "type": "rss"
    },
    "jiqizhixin": {
        "name": "æœºå™¨ä¹‹å¿ƒ",
        "rss": "https://www.jiqizhixin.com/rss",
        "type": "rss"
    },
    "infoq": {
        "name": "InfoQ",
        "rss": "https://www.infoq.cn/feed",
        "type": "rss"
    },
    "36kr": {
        "name": "36æ°ª",
        "rss": "https://36kr.com/feed",
        "type": "rss"
    },
    "leiphone": {
        "name": "é›·é”‹ç½‘",
        "rss": "https://www.leiphone.com/feed",
        "type": "rss"
    },
    "tmtpost": {
        "name": "é’›åª’ä½“",
        "rss": "https://www.tmtpost.com/feed",
        "type": "rss"
    },
    "aiera": {
        "name": "æ–°æ™ºå…ƒ",
        "url": "https://www.aiera.com.cn",
        "type": "web"
    }
}

# AIæ ¸å¿ƒå…³é”®è¯ - æ ‡é¢˜å¿…é¡»åŒ…å«è¿™äº›æ‰ç®—AIæ–°é—»
AI_CORE_KEYWORDS = [
    "AI", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "å¤§æ¨¡å‹", "LLM", 
    "ç¥ç»ç½‘ç»œ", "GPT", "ChatGPT", "Claude", "OpenAI", "æ–‡å¿ƒä¸€è¨€", 
    "é€šä¹‰åƒé—®", "æ™ºè°±", "GLM", "æœˆä¹‹æš—é¢", "Kimi", "DeepSeek", 
    "äººå½¢æœºå™¨äºº", "å…·èº«æ™ºèƒ½", "è‡ªåŠ¨é©¾é©¶", "æ— äººé©¾é©¶", "ç®—åŠ›", 
    "èŠ¯ç‰‡", "è‹±ä¼Ÿè¾¾", "NVIDIA", "GPU", "TPU", "AIèŠ¯ç‰‡",
    "ç”Ÿæˆå¼AI", "AIGC", "å¤šæ¨¡æ€", "Agent", "æ™ºèƒ½ä½“", "AGI"
]

# æ¬¡è¦å…³é”®è¯ - ç”¨äºè¾…åŠ©åˆ¤æ–­
AI_SECONDARY_KEYWORDS = [
    "ç™¾åº¦", "é˜¿é‡Œ", "è…¾è®¯", "å­—èŠ‚", "åä¸º", "å°ç±³", "å•†æ±¤", 
    "æ—·è§†", "ä¾å›¾", "äº‘ä»", "å¯’æ­¦çºª", "åœ°å¹³çº¿", "èèµ„", "IPO", "è´¢æŠ¥"
]

# éœ€è¦è¿‡æ»¤çš„æ ‡é¢˜æ¨¡å¼ï¼ˆç»¼åˆæ—©æŠ¥ã€å¨±ä¹å…«å¦ç­‰ï¼‰
FILTER_PATTERNS = [
    r'æ—©æŠ¥|æ™¨æŠ¥|æ™šæŠ¥|æ—¥æŠ¥',  # ç»¼åˆæ–°é—»
    r'è¦é—»æç¤º|ä»Šæ—¥å¤´æ¡',  # æ‚çƒ©æ–°é—»
    r'é™ä»·|è¢«éª‚|éª‚æƒ¨',  # æƒ…ç»ªæ ‡é¢˜
    r'æ¸¸æˆ|ç”µç«|æ‰‹æ¸¸',  # æ¸¸æˆæ–°é—»ï¼ˆé™¤éæ˜ç¡®AIç›¸å…³ï¼‰
    r'æ˜æ˜Ÿ|å¨±ä¹|å…«å¦',  # å¨±ä¹æ–°é—»
]


def is_ai_related(title, summary=""):
    """
    ä¸¥æ ¼æ£€æŸ¥æ˜¯å¦ä¸ AI ç›¸å…³
    è¦æ±‚ï¼šæ ‡é¢˜å¿…é¡»åŒ…å«æ ¸å¿ƒAIå…³é”®è¯
    """
    title_lower = title.lower()
    
    # å…ˆè¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯AIæ–°é—»çš„æ ‡é¢˜æ¨¡å¼
    for pattern in FILTER_PATTERNS:
        if re.search(pattern, title):
            # ä½†å¦‚æœåŒæ—¶æœ‰å¼ºAIå…³é”®è¯ï¼Œä¿ç•™
            has_strong_ai = any(kw.lower() in title_lower for kw in AI_CORE_KEYWORDS)
            if not has_strong_ai:
                return False, 0
    
    # æ ‡é¢˜å¿…é¡»åŒ…å«æ ¸å¿ƒAIå…³é”®è¯
    core_matches = sum(1 for kw in AI_CORE_KEYWORDS if kw.lower() in title_lower)
    
    if core_matches == 0:
        return False, 0
    
    # è®¡ç®—ç›¸å…³åº¦åˆ†æ•°
    score = core_matches * 10
    
    # æ¬¡è¦å…³é”®è¯åŠ åˆ†
    summary_lower = summary.lower()
    for kw in AI_SECONDARY_KEYWORDS:
        if kw.lower() in title_lower:
            score += 3
        if kw.lower() in summary_lower:
            score += 1
    
    return True, score


def is_yesterday(published_str):
    """æ£€æŸ¥æ˜¯å¦æ˜¯æ˜¨å¤©çš„æ–‡ç« """
    try:
        from email.utils import parsedate_to_datetime
        dt = parsedate_to_datetime(published_str)
        yesterday = datetime.now() - timedelta(days=1)
        return dt.date() == yesterday.date()
    except:
        # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œé»˜è®¤åŒ…å«ï¼ˆè®©å†…å®¹ç­›é€‰æ¥å†³å®šï¼‰
        return True


def fetch_article_content(url, source_name):
    """
    æŠ“å–æ–‡ç« æ­£æ–‡ï¼Œæå–çœŸå®æ‘˜è¦
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = resp.apparent_encoding or 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style", "nav", "header", "footer"]):
            script.decompose()
        
        # å°è¯•æ‰¾æ–‡ç« æ­£æ–‡
        content = ""
        
        # å¸¸è§æ–‡ç« å®¹å™¨é€‰æ‹©å™¨
        selectors = [
            'article', '.article-content', '.post-content', '.entry-content',
            '#article-content', '.content-detail', '.article-detail',
            '[class*="content"]', '[class*="article"]'
        ]
        
        for selector in selectors:
            container = soup.select_one(selector)
            if container:
                # è·å–æ®µè½æ–‡æœ¬
                paragraphs = container.find_all(['p', 'div'])
                texts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    # è¿‡æ»¤çŸ­æ–‡æœ¬å’Œå¯¼èˆªç±»å†…å®¹
                    if len(text) > 30 and not any(x in text for x in ['Copyright', 'ç‰ˆæƒ', 'å…è´£å£°æ˜', 'ç›¸å…³é˜…è¯»', 'æ¨èé˜…è¯»']):
                        texts.append(text)
                
                if texts:
                    content = ' '.join(texts[:5])  # å–å‰5æ®µ
                    break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå–æ•´ä¸ªbodyçš„æ–‡æœ¬
        if not content:
            body = soup.find('body')
            if body:
                content = body.get_text(separator=' ', strip=True)
        
        # æ¸…ç†å¹¶æˆªå–
        content = re.sub(r'\s+', ' ', content)
        content = content[:800]  # å…ˆå¤šå–ä¸€ç‚¹ï¼Œåé¢å†ç²¾ä¿®
        
        return content
        
    except Exception as e:
        print(f"    æŠ“å–æ­£æ–‡å¤±è´¥: {e}")
        return ""


def extract_summary(content, max_length=250):
    """
    ä»æ­£æ–‡æå–ç®€æ´æ‘˜è¦
    """
    if not content:
        return ""
    
    # æ¸…ç†å†…å®¹
    content = content.strip()
    
    # å¦‚æœå†…å®¹è¾ƒçŸ­ï¼Œç›´æ¥è¿”å›
    if len(content) <= max_length:
        return content
    
    # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\.\n])', content)
    summary = ""
    for i in range(0, len(sentences), 2):
        sentence = sentences[i]
        punct = sentences[i+1] if i+1 < len(sentences) else ""
        candidate = summary + sentence + punct
        if len(candidate) > max_length:
            break
        summary = candidate
    
    # å¦‚æœæ²¡æ‰¾åˆ°å¥å­è¾¹ç•Œï¼Œç›´æ¥æˆªæ–­
    if not summary:
        summary = content[:max_length-3] + "..."
    
    return summary.strip()


def fetch_rss(source_key, source_config):
    """æŠ“å– RSS æº"""
    items = []
    try:
        print(f"æ­£åœ¨æŠ“å– {source_config['name']}...")
        feed = feedparser.parse(source_config['rss'])
        
        for entry in feed.entries:
            title = entry.get('title', '').strip()
            published = entry.get('published', '')
            url = entry.get('link', '')
            
            if not title or not url:
                continue
            
            # è·å–RSSæ‘˜è¦ï¼ˆå¤‡ç”¨ï¼‰
            rss_summary = entry.get('summary', entry.get('description', ''))
            rss_summary = re.sub(r'<[^>]+>', '', rss_summary)
            
            # æ£€æŸ¥AIç›¸å…³æ€§å’Œè®¡ç®—åˆ†æ•°
            is_ai, score = is_ai_related(title, rss_summary)
            
            if not is_ai:
                continue
            
            print(f"  âœ“ AIç›¸å…³: {title[:40]}...")
            
            # æŠ“å–æ­£æ–‡è·å–çœŸå®æ‘˜è¦
            print(f"    æŠ“å–æ­£æ–‡...", end=" ")
            content = fetch_article_content(url, source_config['name'])
            
            if content:
                summary = extract_summary(content)
                print(f"æˆåŠŸ ({len(summary)}å­—)")
            else:
                #  fallbackåˆ°RSSæ‘˜è¦
                summary = rss_summary[:300] + '...' if len(rss_summary) > 300 else rss_summary
                print(f"å¤±è´¥ï¼Œä½¿ç”¨RSSæ‘˜è¦")
            
            items.append({
                'title': title,
                'url': url,
                'source': source_config['name'],
                'published': published,
                'summary': summary,
                'score': score
            })
            
    except Exception as e:
        print(f"  âœ— é”™è¯¯: {e}")
    
    return items


def fetch_web(source_key, source_config):
    """ç½‘é¡µæŠ“å–ï¼ˆç”¨äºæ²¡æœ‰ RSS çš„ç«™ç‚¹ï¼‰"""
    items = []
    try:
        print(f"æ­£åœ¨æŠ“å– {source_config['name']} (ç½‘é¡µ)...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        resp = requests.get(source_config['url'], headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # æ–°æ™ºå…ƒç½‘ç«™çš„æ–‡ç« åˆ—è¡¨
        articles = soup.find_all('article', limit=15)
        if not articles:
            articles = soup.find_all(['div', 'a'], class_=re.compile('article|post|news|item'), limit=15)
        
        for article in articles:
            try:
                title_tag = article.find(['h1', 'h2', 'h3', 'h4']) or article.find('a')
                title = title_tag.get_text(strip=True) if title_tag else ''
                
                link_tag = article.find('a', href=True)
                url = link_tag['href'] if link_tag else ''
                if url and not url.startswith('http'):
                    url = source_config['url'] + url
                
                if not title or not url:
                    continue
                
                # æ£€æŸ¥AIç›¸å…³æ€§
                is_ai, score = is_ai_related(title, "")
                
                if not is_ai:
                    continue
                
                print(f"  âœ“ AIç›¸å…³: {title[:40]}...")
                
                # æŠ“å–æ­£æ–‡
                print(f"    æŠ“å–æ­£æ–‡...", end=" ")
                content = fetch_article_content(url, source_config['name'])
                
                if content:
                    summary = extract_summary(content)
                    print(f"æˆåŠŸ ({len(summary)}å­—)")
                else:
                    summary = ""
                    print(f"å¤±è´¥")
                
                items.append({
                    'title': title,
                    'url': url,
                    'source': source_config['name'],
                    'published': '',
                    'summary': summary,
                    'score': score
                })
                
            except Exception as e:
                continue
                
    except Exception as e:
        print(f"  âœ— é”™è¯¯: {e}")
    
    return items


def generate_markdown(news_items):
    """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    md = f"""# ğŸ¤– AI æ¯æ—¥æ–°é—» - {yesterday}

å…± {len(news_items)} æ¡ç²¾é€‰

"""
    
    for i, item in enumerate(news_items, 1):
        md += f"""---

## {i}. [{item['source']}] {item['title']}

{item['summary']}

ğŸ”— [é˜…è¯»åŸæ–‡]({item['url']})

"""
    
    md += """---

*AI News Aggregator | æ¯æ—¥æ›´æ–°*
"""
    
    return md


def main():
    print("="*60)
    print("AI å›½å†…åª’ä½“æ–°é—»æŠ“å–ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
    print("="*60)
    
    all_news = []
    
    for key, config in SOURCES.items():
        if config.get('type') == 'web':
            news = fetch_web(key, config)
        else:
            news = fetch_rss(key, config)
        all_news.extend(news)
        print(f"  ä» {config['name']} è·å– {len(news)} æ¡AIæ–°é—»")
        print()
    
    # æŒ‰åˆ†æ•°æ’åºï¼Œå–å‰10æ¡
    all_news.sort(key=lambda x: x['score'], reverse=True)
    top_news = all_news[:10]
    
    print(f"{'='*60}")
    print(f"å…± {len(all_news)} æ¡AIæ–°é—»ï¼Œç­›é€‰å‡º Top {len(top_news)}")
    print(f"{'='*60}\n")
    
    # ç”Ÿæˆ Markdown
    markdown = generate_markdown(top_news)
    
    # ä¿å­˜æ–‡ä»¶
    import os
    os.makedirs('data', exist_ok=True)
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
    filename = f'data/ai_news_{yesterday}.md'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(markdown)
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜: {filename}")
    
    # åŒæ—¶ä¿å­˜ä¸ºæœ€æ–°ç‰ˆæœ¬
    with open('data/ai_news_latest.md', 'w', encoding='utf-8') as f:
        f.write(markdown)
    
    # è¿”å› Markdown å†…å®¹ï¼ˆç”¨äºç›´æ¥æ˜¾ç¤ºï¼‰
    return markdown


if __name__ == "__main__":
    markdown_content = main()
    print("\n" + "="*60)
    print("MARKDOWN æŠ¥å‘Š")
    print("="*60 + "\n")
    print(markdown_content)
