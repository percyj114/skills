# Deep Scout: Multi-Stage Intelligence Pipeline

Search â†’ Filter â†’ Fetch â†’ Synthesize. Turns a natural-language query into a structured research report with full source citations.

## ğŸš€ Usage

```
/deep-scout "Your research question" [--depth 5] [--freshness pw] [--country US] [--style report]
```

### Options
| Flag | Default | Description |
|------|---------|-------------|
| `--depth N` | 5 | Number of URLs to fully fetch (1â€“10) |
| `--freshness` | `pw` | `pd`=past day, `pw`=past week, `pm`=past month, `py`=past year |
| `--country` | `US` | 2-letter country code for Brave search |
| `--language` | `en` | 2-letter language code |
| `--search-count` | 8 | Total results to collect before filtering |
| `--min-score` | 4 | Minimum relevance score to keep (0â€“10) |
| `--style` | `report` | `report` \| `comparison` \| `bullets` \| `timeline` |
| `--dimensions` | `auto` | Comparison dimensions (comma-separated, for `--style comparison`) |
| `--output FILE` | stdout | Write report to file |
| `--no-browser` | â€” | Disable browser fallback |
| `--no-firecrawl` | â€” | Disable Firecrawl fallback |

---

## ğŸ› ï¸ Pipeline â€” Agent Loop Instructions

When this skill is invoked, execute the following four-stage pipeline:

---

### Stage 1: SEARCH

Call `web_search` with:
```
query: <user query>
count: <search_count>
country: <country>
search_lang: <language>
freshness: <freshness>
```

Collect: title, url, snippet for each result.  
If fewer than 3 results returned, retry with `freshness: "py"` (relaxed).

---

### Stage 2: FILTER

Load `prompts/filter.txt`. Replace template vars:
- `{{query}}` â†’ the user's query
- `{{freshness}}` â†’ freshness param
- `{{min_score}}` â†’ min_score param
- `{{results_json}}` â†’ JSON array of search results

Call the LLM with this prompt. Parse the returned JSON array.  
Keep only results where `keep: true`. Sort by score descending.  
Take top `depth` URLs as the fetch list.

**Deduplication:** Max 2 results per root domain (already handled in filter prompt).

---

### Stage 3: FETCH (Tiered Escalation)

For each URL in the filtered list:

**Tier 1 â€” web_fetch (fast):**
```
Call web_fetch(url)
If content length >= 200 chars â†’ accept, trim to max_chars_per_source
```

**Tier 2 â€” Firecrawl (deep/JS):**
```
If Tier 1 fails or returns < 200 chars:
  Run: scripts/firecrawl-wrap.sh <url> <max_chars>
  If output != "FIRECRAWL_UNAVAILABLE" and != "FIRECRAWL_EMPTY" â†’ accept
```

**Tier 3 â€” Browser (last resort):**
```
If Tier 2 fails:
  Call browser(action="open", url=url)
  Call browser(action="snapshot")
  Load prompts/browser-extract.txt, substitute {{query}} and {{max_chars_per_source}}
  Call LLM with snapshot content + extraction prompt
  If output != "FETCH_FAILED:..." â†’ accept
```

**If all tiers fail:** Use the original snippet from Stage 1 search results. Mark as `[snippet only]`.

Store: `{ url: extracted_content }` dict.

---

### Stage 4: SYNTHESIZE

Choose prompt template based on `--style`:
- `report` / `bullets` / `timeline` â†’ `prompts/synthesize-report.txt`
- `comparison` â†’ `prompts/synthesize-comparison.txt`

Replace template vars:
- `{{query}}` â†’ user query
- `{{today}}` â†’ current date (YYYY-MM-DD)
- `{{language}}` â†’ language param
- `{{source_count}}` â†’ number of successfully fetched sources
- `{{dimensions_or_auto}}` â†’ dimensions param (or "auto")
- `{{fetched_content_blocks}}` â†’ build as:
  ```
  [Source 1] (url1)
  <content>
  ---
  [Source 2] (url2)
  <content>
  ```

Call LLM with the filled prompt. The output is the final report.

If `--output FILE` is set, write the report to that file. Otherwise, print to the channel.

---

## âš™ï¸ Configuration

Defaults are in `config.yaml`. Override via CLI flags above.

---

## ğŸ“‚ Project Structure

```
skills/deep-scout/
â”œâ”€â”€ SKILL.md                     â† This file (agent instructions)
â”œâ”€â”€ config.yaml                  â† Default parameter values
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ filter.txt               â† Stage 2: relevance scoring prompt
â”‚   â”œâ”€â”€ synthesize-report.txt    â† Stage 4: report/bullets/timeline synthesis
â”‚   â”œâ”€â”€ synthesize-comparison.txtâ† Stage 4: comparison table synthesis
â”‚   â””â”€â”€ browser-extract.txt      â† Stage 3: browser snapshot extraction
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run.sh                   â† CLI entrypoint (emits pipeline actions)
â”‚   â””â”€â”€ firecrawl-wrap.sh        â† Firecrawl CLI wrapper with fallback handling
â””â”€â”€ examples/
    â””â”€â”€ openclaw-acquisition.md  â† Example output: OpenClaw M&A intelligence
```

---

## ğŸ”§ Error Handling

| Scenario | Handling |
|----------|----------|
| All fetch attempts fail | Use snippet from Stage 1; mark `[snippet only]` |
| Search returns 0 results | Retry with `freshness: py`; error if still 0 |
| Firecrawl not installed | `firecrawl-wrap.sh` outputs `FIRECRAWL_UNAVAILABLE`, skip silently |
| Browser tool unavailable | Skip Tier 3; proceed with available content |
| LLM synthesis exceeds context | Trim sources proportionally, prioritize high-score sources |
| Rate limit on Brave API | Wait 2s, retry once |

---

## ğŸ“‹ Example Outputs

See `examples/openclaw-acquisition.md` for a full sample report.

---

*Deep Scout v0.1.0 Â· OpenClaw Skills Â· clawhub: deep-scout*
