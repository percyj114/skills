#!/usr/bin/env python3
"""YouTubeæœç´¢ç»“æœè§†é¢‘é“¾æ¥æå–å™¨ - YouTube Search Extractor"""

import subprocess
import re
import sys
import argparse
from datetime import datetime
import time

class YouTubeSearchExtractor:
    """YouTubeæœç´¢ç»“æœè§†é¢‘é“¾æ¥æå–å™¨ç±»"""
    
    def __init__(self, headless=True, wait_time=5, max_links=50):
        self.headless = headless
        self.wait_time = wait_time
        self.max_links = max_links
        self.agent_browser_cmd = ['agent-browser']
    
    def search_youtube(self, search_query):
        """
        ä½¿ç”¨agent-browseræ‰§è¡ŒYouTubeæœç´¢
        """
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")
        
        try:
            # å…³é—­ç°æœ‰çš„æµè§ˆå™¨å®ä¾‹
            subprocess.run(self.agent_browser_cmd + ['close'], capture_output=True, text=True)
            
            # æ‰“å¼€æœç´¢é¡µé¢
            search_url = f"https://www.youtube.com/results?search_query={self._url_encode(search_query)}"
            print(f"ğŸ“Œ è®¿é—®æœç´¢é¡µé¢: {search_url}")
            
            result = subprocess.run(
                self.agent_browser_cmd + ['open', search_url],
                capture_output=True, text=True, check=True
            )
            
            if result.returncode == 0:
                print("âœ… æœç´¢é¡µé¢å·²åŠ è½½")
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                print(f"â³ ç­‰å¾… {self.wait_time} ç§’è®©é¡µé¢åŠ è½½...")
                time.sleep(self.wait_time)
                
                # è·å–é¡µé¢HTMLå†…å®¹
                result = subprocess.run(
                    self.agent_browser_cmd + ['get', 'html', 'body'],
                    capture_output=True, text=True, check=True
                )
                
                if result.returncode == 0:
                    print("âœ… é¡µé¢å†…å®¹è·å–æˆåŠŸ")
                    return result.stdout
                
                else:
                    print(f"âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥: {result.returncode}")
                    if result.stderr:
                        print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
                    return None
                    
            else:
                print(f"âŒ è®¿é—®æœç´¢é¡µé¢å¤±è´¥: {result.returncode}")
                if result.stderr:
                    print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
                return None
                
        except subprocess.CalledProcessError as e:
            print(f"âŒ æ‰§è¡Œå‘½ä»¤å¤±è´¥: {e}")
            if e.stderr:
                print(f"é”™è¯¯ä¿¡æ¯: {e.stderr}")
            return None
            
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            print(traceback.format_exc())
            return None
    
    def extract_video_links(self, html_content):
        """
        ä»HTMLå†…å®¹ä¸­æå–è§†é¢‘é“¾æ¥
        """
        print("ğŸ” æ­£åœ¨æå–è§†é¢‘é“¾æ¥...")
        
        video_links = []
        
        patterns = [
            r'href=["\'](/watch\?v=[\w-]+[^"\']*)["\']',
            r'href=["\'](https?://(?:www\.)?youtube\.com/watch\?v=[\w-]+[^"\']*)["\']',
            r'href=["\'](https?://(?:www\.)?youtu\.be/[\w-]+[^"\']*)["\']'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content)
            for match in matches:
                if match.startswith('/watch'):
                    full_url = f"https://www.youtube.com{match}"
                else:
                    full_url = match
                
                if 'v=' in full_url or 'youtu.be/' in full_url:
                    if '?' in full_url:
                        params = full_url.split('?')[1].split('&')
                        filtered_params = []
                        for param in params:
                            if param.startswith('v=') or param.startswith('pp='):
                                filtered_params.append(param)
                        
                        if filtered_params:
                            full_url = full_url.split('?')[0] + '?' + '&'.join(filtered_params)
                
                if full_url not in video_links:
                    video_links.append(full_url)
        
        print(f"âœ… æ‰¾åˆ° {len(video_links)} ä¸ªè§†é¢‘é“¾æ¥")
        return video_links[:self.max_links]
    
    def save_results(self, search_query, video_links, output_file):
        """
        ä¿å­˜æœç´¢ç»“æœ
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ä¿å­˜HTMLå†…å®¹
        try:
            with open(f"{output_file}.html", 'w', encoding='utf-8') as f:
                f.write(self.html_content)
            print(f"âœ… æœç´¢ç»“æœHTMLå·²ä¿å­˜åˆ° {output_file}.html")
        except Exception as e:
            print(f"âŒ ä¿å­˜HTMLå¤±è´¥: {e}")
        
        # ä¿å­˜é“¾æ¥åˆ—è¡¨
        try:
            with open(f"{output_file}_links.txt", 'w', encoding='utf-8') as f:
                f.write(f"# YouTubeæœç´¢ç»“æœï¼š'{search_query}' ({timestamp})\n")
                f.write(f"# æ‰¾åˆ° {len(video_links)} ä¸ªç›¸å…³è§†é¢‘\n")
                f.write(f"\n")
                
                for i, link in enumerate(video_links, 1):
                    f.write(f"{i}. {link}\n")
            
            print(f"âœ… è§†é¢‘é“¾æ¥å·²ä¿å­˜åˆ° {output_file}_links.txt")
            return f"{output_file}_links.txt"
        except Exception as e:
            print(f"âŒ ä¿å­˜é“¾æ¥åˆ—è¡¨å¤±è´¥: {e}")
            return None
    
    def run_search(self, search_query, output_file):
        """
        å®Œæ•´çš„æœç´¢å’Œæå–æµç¨‹
        """
        print(f"ğŸš€ å¼€å§‹æœç´¢ '{search_query}'...")
        
        # 1. æ‰§è¡Œæœç´¢
        self.html_content = self.search_youtube(search_query)
        
        if not self.html_content:
            print("âŒ æœç´¢å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        # 2. æå–è§†é¢‘é“¾æ¥
        video_links = self.extract_video_links(self.html_content)
        
        if not video_links:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„è§†é¢‘é“¾æ¥")
            return False
        
        # 3. ä¿å­˜ç»“æœ
        output_path = self.save_results(search_query, video_links, output_file)
        
        if output_path:
            print(f"\nğŸ¯ ä»»åŠ¡å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_path}")
            return True
        else:
            print("âŒ ä»»åŠ¡å¤±è´¥")
            return False
    
    def _url_encode(self, text):
        """URLç¼–ç """
        import urllib.parse
        return urllib.parse.quote(text)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="YouTubeæœç´¢ç»“æœè§†é¢‘é“¾æ¥æå–å™¨ - YouTube Search Extractor"
    )
    
    parser.add_argument(
        "search_query",
        help="æœç´¢å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼š'hydrasynth å®æˆ˜åº”ç”¨'ï¼‰"
    )
    
    parser.add_argument(
        "output_file",
        help="è¾“å‡ºæ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰"
    )
    
    parser.add_argument(
        "--headless",
        action="store_true",
        help="ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼"
    )
    
    parser.add_argument(
        "--wait-time",
        type=int,
        default=5,
        help="é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤5ç§’ï¼‰"
    )
    
    parser.add_argument(
        "--max-links",
        type=int,
        default=50,
        help="æœ€å¤§æå–é“¾æ¥æ•°é‡ï¼ˆé»˜è®¤50ä¸ªï¼‰"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†è¾“å‡º"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæå–å™¨å®ä¾‹
    extractor = YouTubeSearchExtractor(
        headless=args.headless,
        wait_time=args.wait_time,
        max_links=args.max_links
    )
    
    # æ‰§è¡Œæœç´¢
    success = extractor.run_search(args.search_query, args.output_file)
    
    # æ¸…ç†
    try:
        subprocess.run(['agent-browser', 'close'], capture_output=True, text=True)
    except:
        pass
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())
