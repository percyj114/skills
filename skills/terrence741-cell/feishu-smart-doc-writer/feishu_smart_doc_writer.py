#!/usr/bin/env python3
"""
Feishu Smart Doc Writer - æ”¹è¿›ç‰ˆ
ä½¿ç”¨ OpenClaw å®˜æ–¹å·¥å…·è°ƒç”¨æ–¹å¼
è‡ªåŠ¨åˆ†å—å†™å…¥ + è‡ªåŠ¨è½¬ç§»æ‰€æœ‰æƒ + è‡ªåŠ¨æ›´æ–°ç´¢å¼•
"""

import re
import time
import json
import asyncio
from typing import List, Optional
from dataclasses import dataclass

# å¯¼å…¥ç´¢å¼•ç®¡ç†å™¨
try:
    from .index_manager import IndexManager, add_doc_to_index
except ImportError:
    from index_manager import IndexManager, add_doc_to_index

@dataclass
class ChunkConfig:
    """åˆ†å—é…ç½®"""
    chunk_size: int = 2000          # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    max_retries: int = 3            # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 1.0        # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    show_progress: bool = True      # æ˜¾ç¤ºè¿›åº¦
    convert_tables: bool = True     # è½¬æ¢è¡¨æ ¼ä¸ºæ–‡æœ¬


class ContentChunker:
    """å†…å®¹åˆ†å—å™¨"""
    
    def __init__(self, config: ChunkConfig = None):
        self.config = config or ChunkConfig()
    
    def chunk_content(self, content: str) -> List[str]:
        """
        å°†é•¿å†…å®¹åˆ†å‰²æˆå¤šä¸ªå°å—
        ç­–ç•¥ï¼šæŒ‰æ®µè½åˆ†å‰²ï¼Œå¦‚æœæ®µè½è¶…è¿‡é™åˆ¶ï¼ŒæŒ‰å¥å­åˆ†å‰²
        """
        chunks = []
        current_chunk = ""
        
        # å…ˆå¤„ç†è¡¨æ ¼
        if self.config.convert_tables:
            content = self._convert_tables(content)
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = self._split_paragraphs(content)
        
        for para in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¼šè¶…é™
            if len(current_chunk) + len(para) > self.config.chunk_size:
                # ä¿å­˜å½“å‰å—
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                
                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…é™ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if len(para) > self.config.chunk_size:
                    sub_chunks = self._split_large_paragraph(para)
                    chunks.extend(sub_chunks)
                    current_chunk = ""
                else:
                    current_chunk = para
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        
        # ä¿å­˜æœ€åä¸€å—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _convert_tables(self, content: str) -> str:
        """å°†Markdownè¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬åˆ—è¡¨"""
        table_pattern = r'\|[^\n]+\|\n\|[-:| ]+\|\n((?:\|[^\n]+\|\n)+)'
        
        def convert_table(match):
            table_text = match.group(0)
            lines = table_text.strip().split('\n')
            
            # æå–è¡¨å¤´
            header = [cell.strip() for cell in lines[0].split('|')[1:-1]]
            
            # æå–æ•°æ®è¡Œ
            result = ["ã€è¡¨æ ¼å†…å®¹ã€‘"]
            for line in lines[2:]:  # è·³è¿‡è¡¨å¤´å’Œåˆ†éš”çº¿
                cells = [cell.strip() for cell in line.split('|')[1:-1]]
                if cells and any(cells):  # ç¡®ä¿ä¸æ˜¯ç©ºè¡Œ
                    row_text = ", ".join([f"{h}: {c}" for h, c in zip(header, cells)])
                    result.append(f"- {row_text}")
            
            return "\n".join(result)
        
        return re.sub(table_pattern, convert_table, content)
    
    def _split_paragraphs(self, content: str) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²ï¼Œä¿ç•™æ ‡é¢˜ç»“æ„"""
        lines = content.split('\n')
        paragraphs = []
        current_para = ""
        
        for line in lines:
            stripped = line.strip()
            
            if not stripped:
                if current_para.strip():
                    paragraphs.append(current_para.strip())
                    current_para = ""
                continue
            
            # å¦‚æœæ˜¯æ ‡é¢˜ï¼Œå•ç‹¬æˆæ®µ
            if stripped.startswith('#'):
                if current_para.strip():
                    paragraphs.append(current_para.strip())
                    current_para = ""
                paragraphs.append(stripped)
            else:
                current_para += line + "\n"
        
        if current_para.strip():
            paragraphs.append(current_para.strip())
        
        return paragraphs
    
    def _split_large_paragraph(self, para: str) -> List[str]:
        """åˆ†å‰²å¤§æ®µè½ï¼ˆæŒ‰å¥å­ï¼‰"""
        chunks = []
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.\n])', para)
        current = ""
        
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            if i + 1 < len(sentences):
                sentence += sentences[i + 1]  # åŠ ä¸Šæ ‡ç‚¹
            
            if len(current) + len(sentence) > self.config.chunk_size:
                if current.strip():
                    chunks.append(current.strip())
                current = sentence
            else:
                current += sentence
        
        if current.strip():
            chunks.append(current.strip())
        
        return chunks


class FeishuDocWriter:
    """
    é£ä¹¦æ–‡æ¡£æ™ºèƒ½å†™å…¥å™¨
    ä½¿ç”¨ OpenClaw å®˜æ–¹å·¥å…·è°ƒç”¨æ–¹å¼
    """
    
    def __init__(self, ctx=None, config: ChunkConfig = None):
        """
        åˆå§‹åŒ–
        
        Args:
            ctx: OpenClaw ä¸Šä¸‹æ–‡å¯¹è±¡ï¼ˆåœ¨ Skill ä¸­ä¼ å…¥ï¼‰
            config: åˆ†å—é…ç½®
        """
        self.ctx = ctx
        self.config = config or ChunkConfig()
        self.chunker = ContentChunker(config)
    
    async def write_document(self, title: str, content: str, folder_token: str = None) -> str:
        """
        åˆ›å»ºæ–°æ–‡æ¡£å¹¶å†™å…¥å†…å®¹ï¼ˆè‡ªåŠ¨åˆ†å—ï¼‰
        
        Args:
            title: æ–‡æ¡£æ ‡é¢˜
            content: æ–‡æ¡£å†…å®¹ï¼ˆæ”¯æŒé•¿å†…å®¹ï¼Œè‡ªåŠ¨åˆ†å—ï¼‰
            folder_token: å¯é€‰çš„æ–‡ä»¶å¤¹token
            
        Returns:
            æ–‡æ¡£URL
        """
        if not self.ctx:
            raise ValueError("éœ€è¦æä¾› OpenClaw ä¸Šä¸‹æ–‡å¯¹è±¡ (ctx)")
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºç©ºæ–‡æ¡£ï¼ˆåªä¼ æ ‡é¢˜ï¼‰
        doc_token = await self._create_empty_doc(title, folder_token)
        doc_url = f"https://feishu.cn/docx/{doc_token}"
        
        if self.config.show_progress:
            print(f"âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {doc_url}")
        
        # ç¬¬äºŒæ­¥ï¼šåˆ†æ‰¹è¿½åŠ å†…å®¹
        success = await self._write_content_in_chunks(doc_token, content)
        
        if not success:
            raise Exception("å†™å…¥å†…å®¹å¤±è´¥")
        
        return doc_url
    
    async def append_to_document(self, doc_url: str, content: str) -> bool:
        """
        è¿½åŠ å†…å®¹åˆ°ç°æœ‰æ–‡æ¡£ï¼ˆè‡ªåŠ¨åˆ†å—ï¼‰
        
        Args:
            doc_url: æ–‡æ¡£URL
            content: è¦è¿½åŠ çš„å†…å®¹
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.ctx:
            raise ValueError("éœ€è¦æä¾› OpenClaw ä¸Šä¸‹æ–‡å¯¹è±¡ (ctx)")
        
        doc_token = self._extract_token_from_url(doc_url)
        return await self._write_content_in_chunks(doc_token, content)
    
    async def _create_empty_doc(self, title: str, folder_token: str = None) -> str:
        """åˆ›å»ºç©ºæ–‡æ¡£ï¼Œåªä¼ æ ‡é¢˜"""
        try:
            # ä½¿ç”¨ OpenClaw å®˜æ–¹å·¥å…·è°ƒç”¨æ–¹å¼
            result = await self.ctx.invoke_tool("feishu_doc.create", {
                "title": title,
                "folder_token": folder_token
            })
            
            # æå– doc_token
            if isinstance(result, dict):
                doc_token = result.get("document_id") or result.get("doc_token")
                if doc_token:
                    return doc_token
            
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æå–
            if isinstance(result, str):
                import re
                match = re.search(r'docx/([a-zA-Z0-9]+)', result)
                if match:
                    return match.group(1)
                return result
            
            raise Exception(f"æ— æ³•è§£ææ–‡æ¡£token: {result}")
            
        except Exception as e:
            raise Exception(f"åˆ›å»ºæ–‡æ¡£å¤±è´¥: {e}")
    
    async def _write_content_in_chunks(self, doc_token: str, content: str) -> bool:
        """åˆ†æ‰¹å†™å…¥å†…å®¹"""
        chunks = self.chunker.chunk_content(content)
        
        if self.config.show_progress:
            print(f"ğŸ“ å†…å®¹å·²åˆ†å‰²ä¸º {len(chunks)} å—ï¼Œå¼€å§‹å†™å…¥...")
        
        for i, chunk in enumerate(chunks, 1):
            if self.config.show_progress:
                print(f"  å†™å…¥ç¬¬ {i}/{len(chunks)} å— ({len(chunk)} å­—ç¬¦)...")
            
            success = await self._append_chunk_with_retry(doc_token, chunk)
            
            if not success:
                print(f"âŒ ç¬¬ {i} å—å†™å…¥å¤±è´¥")
                return False
            
            # æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…APIé™æµ
            if i < len(chunks):
                await asyncio.sleep(0.5)
        
        if self.config.show_progress:
            print(f"âœ… å…¨éƒ¨ {len(chunks)} å—å†™å…¥å®Œæˆ")
        
        return True
    
    async def _append_chunk_with_retry(self, doc_token: str, chunk: str) -> bool:
        """å¸¦é‡è¯•çš„è¿½åŠ å†…å®¹"""
        for attempt in range(self.config.max_retries):
            try:
                return await self._append_chunk(doc_token, chunk)
            except Exception as e:
                if self.config.show_progress:
                    print(f"    å°è¯• {attempt + 1}/{self.config.max_retries} å¤±è´¥: {e}")
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(self.config.retry_delay * (attempt + 1))
                else:
                    return False
        return False
    
    async def _append_chunk(self, doc_token: str, chunk: str) -> bool:
        """è¿½åŠ å•å—å†…å®¹"""
        try:
            # ä½¿ç”¨ OpenClaw å®˜æ–¹å·¥å…·è°ƒç”¨æ–¹å¼
            await self.ctx.invoke_tool("feishu_doc.append", {
                "doc_token": doc_token,
                "content": chunk
            })
            return True
        except Exception as e:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {e}")
    
    def _extract_token_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–doc_token"""
        import re
        match = re.search(r'docx/([a-zA-Z0-9]+)', url)
        if match:
            return match.group(1)
        raise ValueError(f"æ— æ³•ä»URLæå–token: {url}")
    
    async def transfer_ownership(self, doc_url: str, owner_openid: str) -> bool:
        """
        è½¬ç§»æ–‡æ¡£æ‰€æœ‰æƒ
        
        Args:
            doc_url: æ–‡æ¡£URL
            owner_openid: æ–°æ‰€æœ‰è€…çš„openid (ä¾‹å¦‚: ou_xxxxxxxx)
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.ctx:
            raise ValueError("éœ€è¦æä¾› OpenClaw ä¸Šä¸‹æ–‡å¯¹è±¡ (ctx)")
        
        doc_token = self._extract_token_from_url(doc_url)
        
        try:
            # ä½¿ç”¨ç›´æ¥ API è°ƒç”¨è½¬ç§»æ‰€æœ‰æƒ
            # 1. å…ˆè·å– tenant_access_token
            token_result = await self.ctx.invoke_tool("exec", {
                "command": "curl -s -X POST 'https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal' \
  -H 'Content-Type: application/json' \
  -d '{\"app_id\": \"cli_a90cb8b1d0f89cb1\", \"app_secret\": \"U2Mlhpds9hbUgZEVkGFhIfYS1vplru5b\"}'"
            })
            
            import json
            import re
            token_data = json.loads(token_result)
            tenant_token = token_data.get("tenant_access_token")
            
            if not tenant_token:
                raise Exception("æ— æ³•è·å– tenant_access_token")
            
            # 2. è°ƒç”¨è½¬ç§»æ‰€æœ‰æƒ API
            transfer_cmd = f"""curl -s -X POST "https://open.feishu.cn/open-apis/drive/v1/permissions/{doc_token}/members/transfer_owner?type=docx" \
  -H "Authorization: Bearer {tenant_token}" \
  -H "Content-Type: application/json" \
  -d '{{"member_type": "openid", "member_id": "{owner_openid}"}}'"""
            
            transfer_result = await self.ctx.invoke_tool("exec", {
                "command": transfer_cmd
            })
            
            result_data = json.loads(transfer_result)
            
            if result_data.get("code") == 0:
                if self.config.show_progress:
                    print(f"âœ… æ–‡æ¡£æ‰€æœ‰æƒå·²è½¬ç§»ç»™ {owner_openid}")
                return True
            else:
                error_msg = result_data.get("msg", "æœªçŸ¥é”™è¯¯")
                if self.config.show_progress:
                    print(f"âš ï¸ æ‰€æœ‰æƒè½¬ç§»å¤±è´¥: {error_msg}")
                return False
            
        except Exception as e:
            if self.config.show_progress:
                print(f"âš ï¸ æ‰€æœ‰æƒè½¬ç§»å¤±è´¥: {e}")
            
            # è½¬ç§»å¤±è´¥ä¸å½±å“æ–‡æ¡£åˆ›å»ºï¼Œè®°å½•é”™è¯¯ä½†ä¸æŠ›å‡º
            return False
    
    async def write_document_with_transfer(
        self, 
        title: str, 
        content: str, 
        folder_token: str = None,
        owner_openid: str = None
    ) -> dict:
        """
        åˆ›å»ºæ–‡æ¡£å¹¶å†™å…¥å†…å®¹ï¼Œå®Œæˆåè‡ªåŠ¨è½¬ç§»æ‰€æœ‰æƒå¹¶æ›´æ–°æœ¬åœ°ç´¢å¼•
        
        Args:
            title: æ–‡æ¡£æ ‡é¢˜
            content: æ–‡æ¡£å†…å®¹
            folder_token: å¯é€‰çš„æ–‡ä»¶å¤¹token
            owner_openid: æ–°æ‰€æœ‰è€…çš„openidï¼Œå¦‚æœæä¾›åˆ™è‡ªåŠ¨è½¬ç§»æ‰€æœ‰æƒ
        
        Returns:
            {
                "doc_url": "...",
                "doc_token": "...",
                "chunks_count": N,
                "owner_transferred": True/False,
                "index_updated": True/False
            }
        """
        # 1. åˆ›å»ºå¹¶å†™å…¥æ–‡æ¡£
        doc_url = await self.write_document(title, content, folder_token)
        
        # 2. æå– doc_token
        doc_token = self._extract_token_from_url(doc_url)
        
        # 3. è®¡ç®—åˆ†å—æ•°
        chunks = self.chunker.chunk_content(content)
        
        # 4. è½¬ç§»æ‰€æœ‰æƒï¼ˆå¦‚æœæä¾›äº† owner_openidï¼‰
        owner_transferred = False
        if owner_openid:
            owner_transferred = await self.transfer_ownership(doc_url, owner_openid)
        
        # 5. ã€å…³é”®ã€‘è‡ªåŠ¨æ›´æ–°æœ¬åœ°ç´¢å¼•
        index_updated = False
        try:
            # ç”Ÿæˆæ‘˜è¦ï¼ˆå–å‰100å­—ï¼‰
            summary = content[:100].replace('\n', ' ') + "..." if len(content) > 100 else content
            
            # è‡ªåŠ¨åˆ†ç±»æ ‡ç­¾
            tags = self._auto_classify_content(content, title)
            
            # æ›´æ–°ç´¢å¼•
            index_updated = add_doc_to_index(
                name=title,
                url=doc_url,
                token=doc_token,
                summary=summary,
                tags=tags,
                owner=owner_openid or ""
            )
            
            if self.config.show_progress and index_updated:
                print(f"âœ… æ–‡æ¡£ç´¢å¼•å·²æ›´æ–°")
            elif self.config.show_progress:
                print(f"âš ï¸ æ–‡æ¡£ç´¢å¼•æ›´æ–°å¤±è´¥ï¼ˆä¸å½±å“æ–‡æ¡£åˆ›å»ºï¼‰")
                
        except Exception as e:
            if self.config.show_progress:
                print(f"âš ï¸ ç´¢å¼•æ›´æ–°å¤±è´¥: {e}ï¼ˆä¸å½±å“æ–‡æ¡£åˆ›å»ºï¼‰")
        
        return {
            "doc_url": doc_url,
            "doc_token": doc_token,
            "chunks_count": len(chunks),
            "owner_transferred": owner_transferred,
            "index_updated": index_updated
        }
    
    def _auto_classify_content(self, content: str, title: str) -> List[str]:
        """æ ¹æ®å†…å®¹è‡ªåŠ¨åˆ†ç±»"""
        tags = []
        text = (title + " " + content).lower()
        
        # å…³é”®è¯æ˜ å°„åˆ°æ ‡ç­¾
        if any(k in text for k in ["ai", "äººå·¥æ™ºèƒ½", "æ¨¡å‹", "gpt", "llm"]):
            tags.append("AIæŠ€æœ¯")
        if any(k in text for k in ["openclaw", "skill", "agent"]):
            tags.append("OpenClaw")
        if any(k in text for k in ["é£ä¹¦", "æ–‡æ¡£", "feishu", "docx"]):
            tags.append("é£ä¹¦æ–‡æ¡£")
        if any(k in text for k in ["ç”µå•†", "tiktok", "alibaba", "ç©å…·"]):
            tags.append("ç”µå•†")
        if any(k in text for k in ["garmin", "strava", "éª‘è¡Œ", "å¥åº·", "è¿åŠ¨"]):
            tags.append("å¥åº·è¿åŠ¨")
        if any(k in text for k in ["å¯¹è¯", "å½’æ¡£", "èŠå¤©è®°å½•"]):
            tags.append("æ¯æ—¥å½’æ¡£")
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæ ‡ç­¾ï¼Œæ·»åŠ é€šç”¨æ ‡ç­¾
        if not tags:
            tags.append("å…¶ä»–")
        
        return tags


# åŒæ­¥åŒ…è£…å‡½æ•°ï¼ˆæ–¹ä¾¿éå¼‚æ­¥ç¯å¢ƒä½¿ç”¨ï¼‰
def write_document_sync(ctx, title: str, content: str, folder_token: str = None, config: ChunkConfig = None) -> str:
    """åŒæ­¥æ–¹å¼å†™å…¥æ–‡æ¡£"""
    writer = FeishuDocWriter(ctx, config)
    return asyncio.run(writer.write_document(title, content, folder_token))


def write_document_with_transfer_sync(
    ctx, 
    title: str, 
    content: str, 
    folder_token: str = None,
    owner_openid: str = None,
    config: ChunkConfig = None
) -> dict:
    """åŒæ­¥æ–¹å¼å†™å…¥æ–‡æ¡£å¹¶è½¬ç§»æ‰€æœ‰æƒ"""
    writer = FeishuDocWriter(ctx, config)
    return asyncio.run(writer.write_document_with_transfer(title, content, folder_token, owner_openid))


def append_to_document_sync(ctx, doc_url: str, content: str, config: ChunkConfig = None) -> bool:
    """åŒæ­¥æ–¹å¼è¿½åŠ æ–‡æ¡£"""
    writer = FeishuDocWriter(ctx, config)
    return asyncio.run(writer.append_to_document(doc_url, content))
